{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Word2Vec\n",
    "from keras.layers import Activation, Embedding, Merge, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter Settings\n",
    "embedding_size = 200\n",
    "epochs_to_train = 10\n",
    "num_neg_samples = 5\n",
    "sampling_factor = 1e-3\n",
    "window_size = 5\n",
    "save_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url):\n",
    "    \"\"\"\n",
    "    Download a file if not present.\n",
    "    \"\"\"\n",
    "    filename = url.split('/')[-1]\n",
    "    path = get_file(filename, url)\n",
    "    return path\n",
    "    \n",
    "\n",
    "def unzip(zip_filename):\n",
    "    \"\"\"\n",
    "    Extract a file from the zipfile\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filename) as f:\n",
    "        for filename in f.namelist():\n",
    "            dirname = os.path.dirname(filename)\n",
    "            f.extract(filename, dirname)\n",
    "            return os.path.abspath(filename)\n",
    "            \n",
    "\n",
    "# Download Data\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "filename = maybe_download(url)\n",
    "text_file = unzip(filename)\n",
    "url = 'http://download.tensorflow.org/data/questions-words.txt'\n",
    "eval_data = maybe_download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f326639fa312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Load Data\n",
    "sentences = word2vec.Text8Corpus(text_file)\n",
    "sentences = [' '.join(sent) for sent in sentences]\n",
    "tokenizer = Tokenizer(filters=base_filter() + \"'\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sentences = tokenizer.texts_to_sequences(sentences)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary:', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    target_word = Sequential()\n",
    "    target_word.add(Embedding(V, embedding_size, input_length=1))\n",
    "\n",
    "    context = Sequential()\n",
    "    context.add(Embedding(V, embedding_size, input_length=1))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Merge([target_word, context], mode='dot', dot_axes=2))\n",
    "    model.add(Reshape((1,), input_shape=(1, 1)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    sampling_table = make_sampling_table(V, sampling_factor=sampling_factor)\n",
    "    for epoch in range(epochs_to_train):\n",
    "        loss = 0.\n",
    "        for i, sent in enumerate(sentences):\n",
    "            print('{}/{}'.format(i, len(sentences)))\n",
    "            couples, labels = skipgrams(sequence=sent, vocabulary_size=V, window_size=window_size,\n",
    "                                        negative_samples=num_neg_samples, sampling_table=sampling_table)\n",
    "            if couples:\n",
    "                words, contexts = zip(*couples)\n",
    "                words = np.array(words, dtype=np.int32)\n",
    "                contexts = np.array(contexts, dtype=np.int32)\n",
    "                y = np.array(labels, dtype=np.int32)\n",
    "                loss += model.train_on_batch([words, contexts], y)\n",
    "        print('num epoch: {} loss: {}'.format(epoch, loss))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(' '.join([str(V - 1), str(embedding_size)]))\n",
    "        f.write('\\n')\n",
    "        vectors = model.get_weights()[0]\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            f.write(word)\n",
    "            f.write(' ')\n",
    "            f.write(' '.join(map(str, list(vectors[i, :]))))\n",
    "            f.write('\\n')\n",
    "            \n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analogies(filename, word2id):\n",
    "    \"\"\"\n",
    "    Reads through the analogy question file.\n",
    "\n",
    "    Returns:\n",
    "      questions: a [n, 4] numpy array containing the analogy question's word ids.\n",
    "      questions_skipped: questions skipped due to unknown words.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    questions_skipped = 0\n",
    "    with open(filename, 'r') as analogy_f:\n",
    "        for line in analogy_f:\n",
    "            if line.startswith(':'):  # Skip comments.\n",
    "                continue\n",
    "            words = line.strip().lower().split()\n",
    "            ids = [w in word2id for w in words]\n",
    "            if False in ids or len(ids) != 4:\n",
    "                questions_skipped += 1\n",
    "            else:\n",
    "                questions.append(words)\n",
    "    print('Eval analogy file: {}'.format(filename))\n",
    "    print('Questions: {}'.format(len(questions)))\n",
    "    print('Skipped: {}'.format(questions_skipped))\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    w2v = Word2Vec.load_word2vec_format(save_path, binary=False)\n",
    "    word2id = dict([(w, i) for i, w in enumerate(w2v.index2word)])\n",
    "    analogy_questions = read_analogies(eval_data, word2id)\n",
    "    correct = 0\n",
    "    total = len(analogy_questions)\n",
    "    for question in analogy_questions:\n",
    "        a, b, c, d = question  # E.g. [Athens, Greece, Baghdad, Iraq]\n",
    "        analogies = w2v.most_similar(positive=[b, c], negative=[a], topn=4)\n",
    "        for analogy in analogies:\n",
    "            word, _ = analogy\n",
    "            if d == word:\n",
    "                # Predicted Correctly!\n",
    "                correct += 1\n",
    "                break\n",
    "    print('Eval %4d/%d accuracy = %4.1f%%' % (correct, total, correct * 100.0 / total))\n",
    "    \n",
    "eval_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}